{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"conv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduce experience replay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple('Experience',\n",
    "                                    field_names=['state', 'action',\n",
    "                                                 'next_state', 'reward',\n",
    "                                                 'is_game_on'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, device = 'cuda:0'):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace = False)\n",
    "        \n",
    "        states, actions, next_states, rewards, isgameon = zip(*[self.memory[idx] \n",
    "                                                                for idx in indices])\n",
    "        \n",
    "        return torch.Tensor(states).type(torch.float).to(device), \\\n",
    "               torch.Tensor(actions).type(torch.long).to(device), \\\n",
    "               torch.Tensor(next_states).to(device), \\\n",
    "               torch.Tensor(rewards).to(device), torch.tensor(isgameon).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Networks definition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_nn(nn.Module):\n",
    "    def __init__(self, Ni, Nh1, Nh2, No = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(Ni, Nh1)\n",
    "        self.fc2 = nn.Linear(Nh1, Nh2)\n",
    "        self.fc3 = nn.Linear(Nh2, No)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, classification = False, additional_out=False):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_nn(nn.Module):\n",
    "    \n",
    "    channels = [16, 32, 64]\n",
    "    kernels = [3, 3, 3]\n",
    "    strides = [1, 1, 1]\n",
    "    in_channels = 1\n",
    "    \n",
    "    def __init__(self, rows, cols, n_act):\n",
    "        super().__init__()\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels = self.in_channels,\n",
    "                                            out_channels = self.channels[0],\n",
    "                                            kernel_size = self.kernels[0],\n",
    "                                            stride = self.strides[0]),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels = self.channels[0],\n",
    "                                            out_channels = self.channels[1],\n",
    "                                            kernel_size = self.kernels[1],\n",
    "                                            stride = self.strides[1]),\n",
    "                                  nn.ReLU()\n",
    "                                 )\n",
    "        \n",
    "        size_out_conv = self.get_conv_size(rows, cols)\n",
    "        \n",
    "        self.linear = nn.Sequential(nn.Linear(size_out_conv, rows*cols*2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(rows*cols*2, int(rows*cols/2)),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(int(rows*cols/2), n_act),\n",
    "                                   )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x), self.in_channels, self.rows, self.cols)\n",
    "        out_conv = self.conv(x).view(len(x),-1)\n",
    "        out_lin = self.linear(out_conv)\n",
    "        return out_lin\n",
    "    \n",
    "    def get_conv_size(self, x, y):\n",
    "        out_conv = self.conv(torch.zeros(1,self.in_channels, x, y))\n",
    "        return int(np.prod(out_conv.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleQloss(batch, Q1_net, Q2_net, gamma=0.95, device=\"cuda\"):\n",
    "    states, actions, next_states, rewards, _ = batch\n",
    "    lbatch = len(states)\n",
    "    \n",
    "    # Prepare the data\n",
    "    states = states.view(lbatch, -1).to(device)\n",
    "    actions = actions.to(device)\n",
    "    next_states = next_states.view(lbatch, -1).to(device)\n",
    "    rewards = rewards.to(device)\n",
    "    \n",
    "    # Current Q values based on the actions taken\n",
    "    state_action_values = Q1_net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Action selection by Q1_net and value evaluation by Q2_net\n",
    "    # Select the best action in next state according to Q1_net\n",
    "    next_actions = Q1_net(next_states).max(1)[1].unsqueeze(-1)\n",
    "    # Evaluate these actions using Q2_net\n",
    "    next_state_values = Q2_net(next_states).gather(1, next_actions).squeeze(-1)\n",
    "    \n",
    "    # Detach next state values from the graph to prevent gradients from flowing\n",
    "    next_state_values = next_state_values.detach()\n",
    "    \n",
    "    # Calculate expected Q values for the current states\n",
    "    expected_state_action_values = (next_state_values * gamma) + rewards\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the maze and define the environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "from environment import MazeEnvironment\n",
    "maze_data = loadmat('Maze_DQN.mat')  # Ensure this matches the structure of the .mat file\n",
    "maze = maze_data['Maze']\n",
    "\n",
    "initial_position = [16,4]\n",
    "goal = [3, 13]\n",
    "\n",
    "maze_env = MazeEnvironment(maze, initial_position, goal)\n",
    "\n",
    "maze_env.draw_current()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the agent and the buffer for experience replay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_capacity = 10000\n",
    "buffer_start_size = 1000\n",
    "memory_buffer = ExperienceReplay(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent(maze = maze_env,\n",
    "              memory_buffer = memory_buffer,\n",
    "              use_softmax = False\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified code for Double Q-Learning\n",
    "if model_type == \"conv\":\n",
    "    Q1_net = conv_nn(len(maze), len(maze), 4)\n",
    "    Q2_net = conv_nn(len(maze), len(maze), 4)\n",
    "    averaged_net =  conv_nn(len(maze), len(maze), 4)\n",
    "else:\n",
    "    Q1_net= fc_nn(maze.size, maze.size, maze.size, 4)\n",
    "    Q2_net= fc_nn(maze.size, maze.size, maze.size, 4)\n",
    "    averaged_net= fc_nn(maze.size, maze.size, maze.size, 4)\n",
    "\n",
    "optimizer_Q1 = optim.Adam(Q1_net.parameters(), lr=1e-4)\n",
    "optimizer_Q2 = optim.Adam(Q2_net.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "batch_size = 24\n",
    "gamma = 0.95\n",
    "\n",
    "Q1_net.to(device)\n",
    "Q2_net.to(device)\n",
    "averaged_net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the epsilon profile and plot the resetting probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20000\n",
    "\n",
    "cutoff = 3000\n",
    "epsilon = np.exp(-np.arange(num_epochs)/(cutoff))\n",
    "epsilon[epsilon > epsilon[100*int(num_epochs/cutoff)]] = epsilon[100*int(num_epochs/cutoff)]\n",
    "plt.plot(epsilon, color = 'orangered', ls = '--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.savefig('epsilon_profile.pdf', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "best_loss = 1e5\n",
    "previous_running_loss = 1e5  # Initialize with a high value\n",
    "running_loss = 0\n",
    "loss_diff = 1e5\n",
    "average_rewards_log = []\n",
    "estop = \"N/A\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    counter = 0\n",
    "    eps = epsilon[epoch]\n",
    "    \n",
    "    agent.isgameon = True\n",
    "    _ = agent.env.reset(eps)\n",
    "    \n",
    "    while agent.isgameon:\n",
    "        agent.make_a_move_doubleQ(Q1_net, Q2_net, eps,counter)\n",
    "        counter += 1\n",
    "\n",
    "        if len(agent.buffer) < buffer_start_size:\n",
    "            continue\n",
    "            \n",
    "        # Sample from replay buffer\n",
    "        batch = agent.buffer.sample(batch_size, device=device)\n",
    "        \n",
    "        # Decide which network to update\n",
    "        if random.random() > 0.5:\n",
    "            optimizer = optimizer_Q1\n",
    "            loss_t = DoubleQloss(batch, Q1_net, Q2_net, gamma=gamma, device=device)\n",
    "            Q1_net.zero_grad()\n",
    "        else:\n",
    "            optimizer = optimizer_Q2\n",
    "            loss_t = DoubleQloss(batch, Q2_net, Q1_net, gamma=gamma, device=device)\n",
    "            Q2_net.zero_grad()\n",
    "        \n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += loss_t.item()\n",
    "    \n",
    "    if (agent.env.current_position == agent.env.goal).all():\n",
    "        result = 'won'\n",
    "    else:\n",
    "        result = 'lost'\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        # Retrieve state dictionaries\n",
    "        Q1_state_dict = Q1_net.state_dict()\n",
    "        Q2_state_dict = Q2_net.state_dict()\n",
    "\n",
    "        # Use dictionary comprehension to average the parameters\n",
    "        averaged_state_dict = {key: (Q1_state_dict[key] + Q2_state_dict[key]) / 2.0 for key in Q1_state_dict}\n",
    "\n",
    "        # Apply the averaged state dict to the new network\n",
    "        averaged_net.load_state_dict(averaged_state_dict)\n",
    "\n",
    "        agent.plot_policy_map(Q1_net, f'Results\\\\DoubleQ1_{model_type}_sol_epoch_{epoch}.pdf', [0, 0],f'Gamma ({gamma}), Buffer Capacity ({buffer_capacity}), Batch Size ({batch_size}), Network Type ({model_type})',epoch,title=\"Double Q\")\n",
    "        agent.plot_policy_map(Q2_net, f'Results\\\\DoubleQ2_{model_type}_sol_epoch_{epoch}.pdf', [0, 0],f'Gamma ({gamma}), Buffer Capacity ({buffer_capacity}), Batch Size ({batch_size}), Network Type ({model_type})',epoch,title=\"Double Q\")\n",
    "        agent.plot_policy_map(Q2_net, f'Results\\\\DoubleQ_averaged_{model_type}_sol_epoch_{epoch}.pdf', [0, 0],f'Gamma ({gamma}), Buffer Capacity ({buffer_capacity}), Batch Size ({batch_size}), Network Type ({model_type})',epoch,title=\"Double Q\")\n",
    "        agent.plot_Q_values(averaged_net, f'Results\\\\DoubleQ_{model_type}_sol_epoch_{epoch}_Q_values.pdf', [0, 0],f'Gamma ({gamma}), Buffer Capacity ({buffer_capacity}), Batch Size ({batch_size}), Network Type ({model_type})',epoch,title=\"Double Q\")\n",
    "\n",
    "    loss_log.append(loss)\n",
    "    \n",
    "    # Evaluate average loss every 50 epochs after 2000 epochs\n",
    "    \n",
    "    if epoch > 2000 and epoch % 50 == 0:\n",
    "        running_loss = np.mean(loss_log[-50:])\n",
    "        # Update best loss and save the model if there's an improvement\n",
    "        if running_loss < best_loss:\n",
    "            best_loss = running_loss\n",
    "            torch.save(Q1_net.state_dict(), f\"Models\\\\best_Q1_{model_type}.torch\")\n",
    "            torch.save(Q2_net.state_dict(), f\"Models\\\\best_Q2_{model_type}.torch\")\n",
    "\n",
    "            # Retrieve state dictionaries\n",
    "            Q1_state_dict = Q1_net.state_dict()\n",
    "            Q2_state_dict = Q2_net.state_dict()\n",
    "\n",
    "            # Use dictionary comprehension to average the parameters\n",
    "            averaged_state_dict = {key: (Q1_state_dict[key] + Q2_state_dict[key]) / 2.0 for key in Q1_state_dict}\n",
    "\n",
    "            # Apply the averaged state dict to the new network\n",
    "            averaged_net.load_state_dict(averaged_state_dict)\n",
    "\n",
    "            torch.save(averaged_net.state_dict(), \"Models\\\\best_averaged_{model_type}.torch\")\n",
    "            \n",
    "            estop = epoch\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        # Calculate and log the average reward for this run\n",
    "        counter =0\n",
    "        agent.isgameon = True\n",
    "        _ = agent.env.reset_to_starting_tile()\n",
    "\n",
    "        while agent.isgameon:\n",
    "            agent.make_a_move_doubleQ(Q1_net,Q2_net, eps,counter, testing = True)\n",
    "            counter += 1\n",
    "\n",
    "        average_reward = agent.final_total_reward / counter if counter else 0\n",
    "        average_rewards_log.append(average_reward)\n",
    "    \n",
    "    print('Epoch', epoch, '(number of moves ' + str(counter) + ')')\n",
    "    print('Game', result)\n",
    "    print('[' + '#'*(100-int(100*(1 - epoch/num_epochs))) +\n",
    "          ' '*int(100*(1 - epoch/num_epochs)) + ']')\n",
    "    print('\\t Average loss: ' + f'{loss:.5f}')\n",
    "    print('\\t Average Accumulated Reward: ' + f'{average_rewards_log[-1]:.5f}')\n",
    "    if epoch > 2000:\n",
    "        print('\\t Best running average loss: ' + f'{best_loss:.5f}' + ', achieved at epoch', estop)\n",
    "        print('\\t Running loss: ' + f'{running_loss:.5f}')\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(averaged_net.state_dict(), \"Models\\\\averaged_net.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_with_reward = range(0, num_epochs, 20)[:len(average_rewards_log)]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_with_reward, average_rewards_log, linestyle='-', color='blue')\n",
    "plt.title(f'Average Accumulated Reward vs Epoch (Double DQN - {model_type})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Accumulated Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the maze solution and the policy learnt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_net.eval()\n",
    "agent.isgameon = True\n",
    "agent.use_softmax = False\n",
    "_ = agent.env.reset(0)\n",
    "while agent.isgameon:\n",
    "    agent.make_a_move(averaged_net, 0)\n",
    "    agent.env.draw('')\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_map(averaged_net, 'Results\\\\DoubleQ_solution.pdf', [0,0], f'Gamma ({gamma}), Buffer Capacity ({buffer_capacity}), Batch Size ({batch_size}), Network Type ({model_type})',epoch,title=\"Double Q\")\n",
    "agent.plot_Q_values(averaged_net, 'Results\\\\DoubleQ_solution_Values.pdf', [0, 0],f'Gamma ({gamma}), Buffer Capacity ({buffer_capacity}), Batch Size ({batch_size}), Network Type ({model_type})',epoch,title=\"Double Q\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
